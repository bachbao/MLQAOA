{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1865aff-a716-4115-8f65-dfb24f6730f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from threading import Thread, Lock\n",
    "\n",
    "# Thread-safe centroid update\n",
    "class ThreadSafeCentroids:\n",
    "    def __init__(self, centroids):\n",
    "        self.centroids = centroids\n",
    "        self.lock = Lock()\n",
    "\n",
    "    def update(self, updates):\n",
    "        with self.lock:\n",
    "            for i, (new_sum, count) in updates.items():\n",
    "                if count > 0:\n",
    "                    self.centroids[i] = new_sum / count\n",
    "\n",
    "# K-means clustering with threading\n",
    "def kmeans_threaded(data, k, init_centroids = None, \n",
    "                    max_iterations=100, num_threads=1):\n",
    "    n_samples, n_features = data.shape\n",
    "    if init_centroids is None:\n",
    "        centroids = data[np.random.choice(n_samples, k, replace=False)]\n",
    "    else:\n",
    "        centroids = init_centroids\n",
    "    thread_safe_centroids = ThreadSafeCentroids(centroids)\n",
    "\n",
    "    def assign_and_update(chunk, thread_id, chunk_updates):\n",
    "        chunk_centroids = np.zeros((k, n_features))\n",
    "        counts = np.zeros(k)\n",
    "\n",
    "        distances = np.linalg.norm(chunk[:, None] - centroids[None, :], axis=2)\n",
    "        clusters = np.argmin(distances, axis=1)\n",
    "\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            chunk_centroids[cluster] += chunk[i]\n",
    "            counts[cluster] += 1\n",
    "\n",
    "        chunk_updates[thread_id] = {i: (chunk_centroids[i], counts[i]) for i in range(k)}\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        # Step 1: Divide data into chunks\n",
    "        chunk_size = n_samples // num_threads\n",
    "        threads = []\n",
    "        chunk_updates = [None] * num_threads\n",
    "\n",
    "        # Step 2: Create and start threads\n",
    "        for i in range(num_threads):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size if i != num_threads - 1 else n_samples\n",
    "            chunk = data[start:end]\n",
    "            thread = Thread(target=assign_and_update, args=(chunk, i, chunk_updates))\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "\n",
    "        # Step 3: Wait for all threads to finish\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "        # Step 4: Update centroids\n",
    "        global_updates = {}\n",
    "        for updates in chunk_updates:\n",
    "            for cluster_id, (new_sum, count) in updates.items():\n",
    "                if cluster_id not in global_updates:\n",
    "                    global_updates[cluster_id] = (new_sum, count)\n",
    "                else:\n",
    "                    current_sum, current_count = global_updates[cluster_id]\n",
    "                    global_updates[cluster_id] = (current_sum + new_sum, current_count + count)\n",
    "\n",
    "        thread_safe_centroids.update(global_updates)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.allclose(centroids, thread_safe_centroids.centroids):\n",
    "            break\n",
    "        centroids = np.copy(thread_safe_centroids.centroids)\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9cb725a-b505-4aa8-8f1e-9ebc448eb61b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from multiprocessing import Process, Manager, Array\n",
    "import ctypes\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# K-means clustering with multiprocessing\n",
    "def kmeans_multiprocess(data, k, init_centroids=None,\n",
    "                        max_iterations=100, num_processes=4):\n",
    "    n_samples, n_features = data.shape\n",
    "    data = np.array(data, dtype=np.float64)\n",
    "\n",
    "    # Initialize centroids\n",
    "    if init_centroids is None:\n",
    "        centroids = data[np.random.choice(n_samples, k, replace=False)]\n",
    "    else:\n",
    "        centroids = init_centroids    \n",
    "    shared_centroids = Array(ctypes.c_double, centroids.flatten(), lock=False)\n",
    "\n",
    "    def assign_and_update(chunk, return_dict, process_id):\n",
    "        local_centroids = np.frombuffer(shared_centroids).reshape((k, n_features))\n",
    "        chunk_centroids = np.zeros((k, n_features))\n",
    "        counts = np.zeros(k)\n",
    "\n",
    "        # Compute distances and assign clusters\n",
    "        distances = cdist(chunk, local_centroids)\n",
    "        clusters = np.argmin(distances, axis=1)\n",
    "\n",
    "        # Compute chunk centroids\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            chunk_centroids[cluster] += chunk[i]\n",
    "            counts[cluster] += 1\n",
    "\n",
    "        # Store updates in a shared dictionary\n",
    "        return_dict[process_id] = (chunk_centroids, counts)\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        # Divide data into chunks\n",
    "        chunk_size = n_samples // num_processes\n",
    "        processes = []\n",
    "        manager = Manager()\n",
    "        return_dict = manager.dict()\n",
    "\n",
    "        # Start processes\n",
    "        for i in range(num_processes):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size if i != num_processes - 1 else n_samples\n",
    "            chunk = data[start:end]\n",
    "            process = Process(target=assign_and_update, args=(chunk, return_dict, i))\n",
    "            processes.append(process)\n",
    "            process.start()\n",
    "\n",
    "        # Wait for all processes to complete\n",
    "        for process in processes:\n",
    "            process.join()\n",
    "\n",
    "        # Aggregate updates\n",
    "        global_centroids = np.zeros((k, n_features))\n",
    "        global_counts = np.zeros(k)\n",
    "        for chunk_centroids, counts in return_dict.values():\n",
    "            global_centroids += chunk_centroids\n",
    "            global_counts += counts\n",
    "\n",
    "        # Update centroids\n",
    "        for i in range(k):\n",
    "            if global_counts[i] > 0:\n",
    "                centroids[i] = global_centroids[i] / global_counts[i]\n",
    "\n",
    "        # Update shared centroids\n",
    "        np.copyto(np.frombuffer(shared_centroids).reshape((k, n_features)), centroids)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.allclose(centroids, np.frombuffer(shared_centroids).reshape((k, n_features))):\n",
    "            break\n",
    "\n",
    "    return np.array(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abae95e2-e116-4809-ab65-450a6f645d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = np.random.rand(10_000_000, 10)\n",
    "k = 3\n",
    "init_centroids = centroids = data[np.random.choice(data.shape[0], k, replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c46056f7-fbd7-4282-be7e-b0ed9eec2df5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c127e465-8189-4d9c-b1fc-757874d3020f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute in 12.636538830585778 seconds\n"
     ]
    }
   ],
   "source": [
    "st = time.perf_counter()\n",
    "centroids = kmeans_threaded(data, k, \n",
    "                            init_centroids.copy(), \n",
    "                            max_iterations=100, \n",
    "                            num_threads=1)\n",
    "et = time.perf_counter()\n",
    "print(f\"Execute in {et-st} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58a049b5-5790-455a-8795-9a7eb6e82998",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49395107, 0.57331377, 0.52462958, 0.3068108 , 0.38209598,\n",
       "        0.60287094, 0.35794831, 0.67290944, 0.49404066, 0.34783139],\n",
       "       [0.60510198, 0.4776329 , 0.60299206, 0.47767964, 0.55563296,\n",
       "        0.26324388, 0.36862252, 0.53048049, 0.44516577, 0.63539886],\n",
       "       [0.45112466, 0.48017385, 0.43905172, 0.59177912, 0.52225121,\n",
       "        0.57350185, 0.62429002, 0.41273632, 0.52935063, 0.49725408]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "956e8b81-044a-4c84-b554-fa33179b027e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute in 4.03635769803077 seconds\n"
     ]
    }
   ],
   "source": [
    "st = time.perf_counter()\n",
    "centroids = kmeans_multiprocess(data, k, \n",
    "                            init_centroids.copy(), \n",
    "                            max_iterations=100, \n",
    "                            num_processes=3)\n",
    "et = time.perf_counter()\n",
    "print(f\"Execute in {et-st} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a88dff8c-8b8e-41ce-8270-6fe5491bfb6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49395107, 0.57331377, 0.52462958, 0.3068108 , 0.38209598,\n",
       "        0.60287094, 0.35794831, 0.67290944, 0.49404066, 0.34783139],\n",
       "       [0.60510198, 0.4776329 , 0.60299206, 0.47767964, 0.55563296,\n",
       "        0.26324388, 0.36862252, 0.53048049, 0.44516577, 0.63539886],\n",
       "       [0.45112466, 0.48017385, 0.43905172, 0.59177912, 0.52225121,\n",
       "        0.57350185, 0.62429002, 0.41273632, 0.52935063, 0.49725408]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c9623596-f54d-47a2-9c98-c6e9ab40d574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2a12af12-a962-4305-b8f0-f5ada5b0315c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute in 9.78438676521182 seconds\n"
     ]
    }
   ],
   "source": [
    "st = time.perf_counter()\n",
    "kmeans = KMeans(n_clusters=k, \n",
    "                init=init_centroids.copy(), \n",
    "                max_iter=100, tol=1e-8).fit(data)\n",
    "et = time.perf_counter()\n",
    "print(f\"Execute in {et-st} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "77c8a087-7af2-4d7c-a344-4123979a2a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.50046925, 0.50049585, 0.49996786, 0.49906113, 0.50097786,\n",
       "        0.68321648, 0.23204047, 0.49996228, 0.49980606, 0.50041033],\n",
       "       [0.50017562, 0.4993498 , 0.49960967, 0.50060317, 0.49988174,\n",
       "        0.19631078, 0.49942716, 0.50026178, 0.50031572, 0.49963537],\n",
       "       [0.49975323, 0.5001165 , 0.50050785, 0.49996848, 0.49949139,\n",
       "        0.68230043, 0.76893009, 0.50001688, 0.49964111, 0.50011759]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8495e139-e7af-427f-9a95-d28326465259",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML_maxcut]",
   "language": "python",
   "name": "conda-env-ML_maxcut-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
