{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1865aff-a716-4115-8f65-dfb24f6730f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from threading import Thread, Lock\n",
    "\n",
    "# Thread-safe centroid update\n",
    "class ThreadSafeCentroids:\n",
    "    def __init__(self, centroids):\n",
    "        self.centroids = centroids\n",
    "        self.lock = Lock()\n",
    "\n",
    "    def update(self, updates):\n",
    "        with self.lock:\n",
    "            for i, (new_sum, count) in updates.items():\n",
    "                if count > 0:\n",
    "                    self.centroids[i] = new_sum / count\n",
    "\n",
    "# K-means clustering with threading\n",
    "def kmeans_threaded(data, k, init_centroids = None, \n",
    "                    max_iterations=100, num_threads=1):\n",
    "    n_samples, n_features = data.shape\n",
    "    if init_centroids is None:\n",
    "        centroids = data[np.random.choice(n_samples, k, replace=False)]\n",
    "    else:\n",
    "        centroids = init_centroids\n",
    "    thread_safe_centroids = ThreadSafeCentroids(centroids)\n",
    "\n",
    "    def assign_and_update(chunk, thread_id, chunk_updates):\n",
    "        chunk_centroids = np.zeros((k, n_features))\n",
    "        counts = np.zeros(k)\n",
    "\n",
    "        distances = np.linalg.norm(chunk[:, None] - centroids[None, :], axis=2)\n",
    "        clusters = np.argmin(distances, axis=1)\n",
    "\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            chunk_centroids[cluster] += chunk[i]\n",
    "            counts[cluster] += 1\n",
    "\n",
    "        chunk_updates[thread_id] = {i: (chunk_centroids[i], counts[i]) for i in range(k)}\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        # Step 1: Divide data into chunks\n",
    "        chunk_size = n_samples // num_threads\n",
    "        threads = []\n",
    "        chunk_updates = [None] * num_threads\n",
    "\n",
    "        # Step 2: Create and start threads\n",
    "        for i in range(num_threads):\n",
    "            start = i * chunk_size\n",
    "            end = (i + 1) * chunk_size if i != num_threads - 1 else n_samples\n",
    "            chunk = data[start:end]\n",
    "            thread = Thread(target=assign_and_update, args=(chunk, i, chunk_updates))\n",
    "            threads.append(thread)\n",
    "            thread.start()\n",
    "\n",
    "        # Step 3: Wait for all threads to finish\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "        # Step 4: Update centroids\n",
    "        global_updates = {}\n",
    "        for updates in chunk_updates:\n",
    "            for cluster_id, (new_sum, count) in updates.items():\n",
    "                if cluster_id not in global_updates:\n",
    "                    global_updates[cluster_id] = (new_sum, count)\n",
    "                else:\n",
    "                    current_sum, current_count = global_updates[cluster_id]\n",
    "                    global_updates[cluster_id] = (current_sum + new_sum, current_count + count)\n",
    "\n",
    "        thread_safe_centroids.update(global_updates)\n",
    "\n",
    "        # Check for convergence\n",
    "        if np.allclose(centroids, thread_safe_centroids.centroids):\n",
    "            break\n",
    "        centroids = np.copy(thread_safe_centroids.centroids)\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e9cb725a-b505-4aa8-8f1e-9ebc448eb61b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from multiprocessing import Process, Manager, Array\n",
    "# import ctypes\n",
    "# from scipy.spatial.distance import cdist\n",
    "\n",
    "# # K-means clustering with multiprocessing\n",
    "# def kmeans_multiprocess(data, k, init_centroids=None,\n",
    "#                         max_iterations=100, num_processes=4):\n",
    "#     n_samples, n_features = data.shape\n",
    "#     data = np.array(data, dtype=np.float64)\n",
    "\n",
    "#     # Initialize centroids\n",
    "#     if init_centroids is None:\n",
    "#         centroids = data[np.random.choice(n_samples, k, replace=False)]\n",
    "#     else:\n",
    "#         centroids = init_centroids    \n",
    "#     shared_centroids = Array(ctypes.c_double, centroids.flatten(), lock=False)\n",
    "\n",
    "#     def assign_and_update(chunk, return_dict, process_id):\n",
    "#         local_centroids = np.frombuffer(shared_centroids).reshape((k, n_features))\n",
    "#         chunk_centroids = np.zeros((k, n_features))\n",
    "#         counts = np.zeros(k)\n",
    "\n",
    "#         # Compute distances and assign clusters\n",
    "#         distances = cdist(chunk, local_centroids)\n",
    "#         clusters = np.argmin(distances, axis=1)\n",
    "\n",
    "#         # Compute chunk centroids\n",
    "#         for i, cluster in enumerate(clusters):\n",
    "#             chunk_centroids[cluster] += chunk[i]\n",
    "#             counts[cluster] += 1\n",
    "\n",
    "#         # Store updates in a shared dictionary\n",
    "#         return_dict[process_id] = (chunk_centroids, counts)\n",
    "\n",
    "#     for _ in range(max_iterations):\n",
    "#         # Divide data into chunks\n",
    "#         chunk_size = n_samples // num_processes\n",
    "#         processes = []\n",
    "#         manager = Manager()\n",
    "#         return_dict = manager.dict()\n",
    "\n",
    "#         # Start processes\n",
    "#         for i in range(num_processes):\n",
    "#             start = i * chunk_size\n",
    "#             end = (i + 1) * chunk_size if i != num_processes - 1 else n_samples\n",
    "#             chunk = data[start:end]\n",
    "#             process = Process(target=assign_and_update, args=(chunk, return_dict, i))\n",
    "#             processes.append(process)\n",
    "#             process.start()\n",
    "\n",
    "#         # Wait for all processes to complete\n",
    "#         for process in processes:\n",
    "#             process.join()\n",
    "\n",
    "#         # Aggregate updates\n",
    "#         global_centroids = np.zeros((k, n_features))\n",
    "#         global_counts = np.zeros(k)\n",
    "#         for chunk_centroids, counts in return_dict.values():\n",
    "#             global_centroids += chunk_centroids\n",
    "#             global_counts += counts\n",
    "\n",
    "#         # Update centroids\n",
    "#         for i in range(k):\n",
    "#             if global_counts[i] > 0:\n",
    "#                 centroids[i] = global_centroids[i] / global_counts[i]\n",
    "\n",
    "#         # Update shared centroids\n",
    "#         np.copyto(np.frombuffer(shared_centroids).reshape((k, n_features)), centroids)\n",
    "\n",
    "#         # Check for convergence\n",
    "#         if np.allclose(centroids, np.frombuffer(shared_centroids).reshape((k, n_features))):\n",
    "#             break\n",
    "#     return np.array(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "fae910a7-bc53-495d-9e14-a1f9ac9dbeed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Optimal version\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "from multiprocessing import Process, Array, cpu_count\n",
    "import ctypes\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "# K-means clustering with optimized multiprocessing\n",
    "def kmeans_multiprocess(data, k, init_centroids=None, max_iterations=100, num_processes=None):\n",
    "    n_samples, n_features = data.shape\n",
    "    data = np.array(data, dtype=np.float64)\n",
    "\n",
    "    # Determine the number of processes to use\n",
    "    if num_processes is None:\n",
    "        num_processes = cpu_count()  # Use all available CPU cores by default\n",
    "\n",
    "    # Initialize centroids\n",
    "    if init_centroids is None:\n",
    "        centroids = np.array(data[np.random.choice(n_samples, k, replace=False)], dtype=np.float64)\n",
    "    else:\n",
    "        centroids = init_centroids    \n",
    "    shared_centroids = Array(ctypes.c_double, centroids.flatten(), lock=False)\n",
    "    \n",
    "    def assign_and_update(start_idx, end_idx, return_dict, process_id):\n",
    "        local_centroids = np.frombuffer(shared_centroids).reshape((k, n_features))\n",
    "        chunk_centroids = np.zeros((k, n_features), dtype=np.float64)\n",
    "        counts = np.zeros(k, dtype=np.int64)\n",
    "\n",
    "        # Slice data for this process\n",
    "        chunk = data[start_idx:end_idx]\n",
    "\n",
    "        # Compute distances and assign clusters\n",
    "        # distances = np.zeros((end_idx-start_idx, k))\n",
    "        # for i in range(end_idx-start_idx):\n",
    "        #     for j in range(k):\n",
    "        #         distances[i][j] = norm_square[j]\n",
    "        # distances += -2 * chunk@centroids.T\n",
    "        distances = cdist(chunk, local_centroids)\n",
    "        clusters = np.argmin(distances, axis=1)\n",
    "\n",
    "        # Compute chunk centroids\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            chunk_centroids[cluster] += chunk[i]\n",
    "            counts[cluster] += 1\n",
    "\n",
    "        # Store updates in a shared dictionary\n",
    "        return_dict[process_id] = (chunk_centroids, counts)\n",
    "\n",
    "    for _ in range(max_iterations):\n",
    "        # Divide data into chunks\n",
    "        indices = np.array_split(np.arange(n_samples), num_processes)\n",
    "        processes = []\n",
    "        manager = Manager()\n",
    "        return_dict = manager.dict()\n",
    "        centroids = np.asarray(centroids)\n",
    "        #norms_square = np.einsum(\"ij,ij->i\", centroids, centroids)\n",
    "        \n",
    "        # Start processes\n",
    "        for process_id, chunk_indices in enumerate(indices):\n",
    "            start_idx = chunk_indices[0]\n",
    "            end_idx = chunk_indices[-1] + 1 \n",
    "            process = Process(target=assign_and_update, args=(start_idx, end_idx, \n",
    "                                                              return_dict, process_id))\n",
    "            processes.append(process)\n",
    "            process.start()\n",
    "\n",
    "        # Wait for all processes to complete\n",
    "        for process in processes:\n",
    "            process.join()\n",
    "\n",
    "        # Aggregate updates\n",
    "        global_centroids = np.zeros((k, n_features), dtype=np.float64)\n",
    "        global_counts = np.zeros(k, dtype=np.int64)\n",
    "        for chunk_centroids, counts in return_dict.values():\n",
    "            global_centroids += chunk_centroids\n",
    "            global_counts += counts\n",
    "            \n",
    "        # Update centroids\n",
    "        for i in range(k):\n",
    "            if global_counts[i] > 0:\n",
    "                centroids[i] = global_centroids[i] / global_counts[i]\n",
    "                \n",
    "        # Check for convergence\n",
    "        if np.allclose(centroids, np.frombuffer(shared_centroids).reshape((k, n_features))):\n",
    "            break\n",
    "            \n",
    "        # Update shared centroids\n",
    "        np.copyto(np.frombuffer(shared_centroids).reshape((k, n_features)), centroids)\n",
    "    return np.array(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "abae95e2-e116-4809-ab65-450a6f645d8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = np.random.rand(20_000_000, 10)\n",
    "k = 3\n",
    "init_centroids = data[np.random.choice(data.shape[0], k, replace=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "c46056f7-fbd7-4282-be7e-b0ed9eec2df5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "192c650e-5ebc-4b45-b859-2bb882cf2aac",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute in 78.68113189283758 seconds\n"
     ]
    }
   ],
   "source": [
    "st = time.perf_counter()\n",
    "centroids = kmeans_multiprocess(data, k, \n",
    "                                init_centroids.copy(), \n",
    "                                max_iterations=50, \n",
    "                                num_processes=os.cpu_count())\n",
    "et = time.perf_counter()\n",
    "print(f\"Execute in {et-st} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "58a049b5-5790-455a-8795-9a7eb6e82998",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49924796, 0.49979747, 0.50028756, 0.4999987 , 0.50003132,\n",
       "        0.76837712, 0.50008134, 0.50006836, 0.50030594, 0.31718857],\n",
       "       [0.49991338, 0.50008928, 0.50011637, 0.49985921, 0.49991163,\n",
       "        0.23156486, 0.49974233, 0.49953336, 0.50074087, 0.31700593],\n",
       "       [0.50065563, 0.50007008, 0.49970823, 0.50009188, 0.50008672,\n",
       "        0.49993602, 0.50049502, 0.50018627, 0.49939285, 0.80364321]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c9623596-f54d-47a2-9c98-c6e9ab40d574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "2a12af12-a962-4305-b8f0-f5ada5b0315c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execute in 15.301100242882967 seconds\n"
     ]
    }
   ],
   "source": [
    "st = time.perf_counter()\n",
    "kmeans = KMeans(n_clusters=k, \n",
    "                init=init_centroids.copy(), \n",
    "                max_iter=50, tol=1e-8).fit(data)\n",
    "et = time.perf_counter()\n",
    "print(f\"Execute in {et-st} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "77c8a087-7af2-4d7c-a344-4123979a2a4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.49924796, 0.49979747, 0.50028756, 0.4999987 , 0.50003132,\n",
       "        0.76837712, 0.50008134, 0.50006836, 0.50030594, 0.31718857],\n",
       "       [0.49991338, 0.50008928, 0.50011637, 0.49985921, 0.49991163,\n",
       "        0.23156486, 0.49974233, 0.49953336, 0.50074087, 0.31700593],\n",
       "       [0.50065563, 0.50007008, 0.49970823, 0.50009188, 0.50008672,\n",
       "        0.49993602, 0.50049502, 0.50018627, 0.49939285, 0.80364321]])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmeans.cluster_centers_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ML_maxcut]",
   "language": "python",
   "name": "conda-env-ML_maxcut-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
